# py torch 中的张量

> 原文:[https://www.geeksforgeeks.org/tensors-in-pytorch/](https://www.geeksforgeeks.org/tensors-in-pytorch/)

Pytorch 张量基本上与 NumPy 数组相同。这意味着它对深度学习或计算图形或梯度一无所知，只是一个用于任意数值计算的通用 n 维数组。然而，NumPy 阵列和 PyTorch Tensor 之间最大的区别是 PyTorch Tensor 可以在中央处理器或图形处理器上运行。要在图形处理器上运行操作，只需使用以下命令将张量转换为 cuda 数据类型:

> 设备= torch . device(“CPU”)
> 
> #要创建随机输入和输出数据，
> 
> #而 H 是隐藏维度；D_out 是输出维度。
> 
> n，D_in，H，D_out = 32，100，10，2
> 
> x = torch.randn(N，D_in，device=device，dtype = torch . float)#其中 x 是张量

在上面的例子中，x 可以被认为是随机特征张量，作为模型的输入。在本文中，我们将看到如何在张量上创建张量、不同的属性和运算。

### **如何创建张量？**

您可以使用一些简单的代码行创建一个张量，如下所示。

## 蟒蛇 3

```
import torch
V_data = [1, 2, 3, 4, 5]
V = torch.tensor(V_data)
print(V)
```

**输出:**

```
tensor([1, 2, 3, 4, 5])
```

您还可以创建给定维度的随机数据张量，如:

## 蟒蛇 3

```
import torch

x = torch.randn((3, 4, 5))
print(x)
```

**输出:**

```
tensor([[[ 0.8332, -0.2102,  0.0213,  0.4375, -0.9506],
         [ 0.0877, -1.5845, -0.1520,  0.3944, -0.7282],
         [-0.6923,  0.0332, -0.4628, -0.9127, -1.4349],
         [-0.3641, -0.5880, -0.5963, -1.4126,  0.5308]],

        [[ 0.4492, -1.2030,  2.5985,  0.8966,  0.4876],
         [ 0.5083,  1.4515,  0.6496,  0.3407,  0.0093],
         [ 0.1237,  0.3783, -0.7969,  1.4019,  0.0633],
         [ 0.4399,  0.3827,  1.2231, -0.0674, -1.0158]],

        [[-0.2490, -0.5475,  0.6201, -2.2092,  0.8405],
         [ 0.1684, -1.0118,  0.7414, -3.3518, -0.3209],
         [ 0.6543,  0.1956, -0.2954,  0.1055,  1.6523],
         [-0.9872, -2.0118, -1.6609,  1.4072,  0.0632]]])
```

您也可以使用以下函数创建张量:

*   **torch.zeros():** 用所有元素创建一个新的 Tensor，初始化为零。

## 蟒蛇 3

```
import torch

z= torch.zeros([3,3], dtype=torch.int32)
print(z)
```

**输出:**

```
tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]], dtype=torch.int32)
```

*   **torch.ones()** :用所有元素创建一个新的 Tensor，初始化为 1。

## 蟒蛇 3

```
import torch

z = torch.ones([3,3])
print(z)
```

**输出:**

```
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
```

*   **torch.full()和 torch.full_like():** 这些函数返回一个所需大小的 Tensor，用提供的所需 fill_value 填充。torch.full()的完整原型是:

> **语法:** torch.full(大小，fill_value，out=None，dtype=None，布局=torch.strided，设备=None，requires_grad=False)

而 *torch.full_like()* 是:

> **语法:** torch.full_like(输入，fill_value，out=None，dtype=None，布局=torch.strided，设备=None，requires_grad=False，memory _ format = torch . preserve _ format)

## 蟒蛇 3

```
import torch

# example of torch.full()
newTensor= torch.full((4, 3), 3.14,dtype= torch.float32)
print(newTensor)
```

**输出:**

```
tensor([[3.1400, 3.1400, 3.1400],
        [3.1400, 3.1400, 3.1400],
        [3.1400, 3.1400, 3.1400],
        [3.1400, 3.1400, 3.1400]])
```

## 蟒蛇 3

```
import torch

# Example for torch.full_like()
x = torch.full_like(newTensor,3.24, dtype=None )
print(x)
```

**输出:**

```
tensor([[3.2400, 3.2400, 3.2400],
        [3.2400, 3.2400, 3.2400],
        [3.2400, 3.2400, 3.2400],
        [3.2400, 3.2400, 3.2400]])
```

这里返回了一个新的 Tensor，其大小和数据类型与前面示例中从*火炬返回的*新 Tensor* 相同。*

### **张量属性:**

每个张量(**火炬。张量**)具有**火炬.数据类型、火炬.设备**和**火炬.布局**属性。

*   **火炬.数据类型**:一个*火炬.数据类型*是代表*火炬数据类型的一个对象。张量*。PyTorch 有 12 种不同的数据类型。
*   **火炬装置**:一个*火炬装置*是一个物体，代表一个*火炬所在的装置。张量 i* s 或将被分配。 *torch.device* 包含设备类型(“cpu”或“cuda”)和设备类型的可选设备序号。

**示例:**

## 蟒蛇 3

```
torch.device('cuda:0')
```

**输出:**

```
device(type='cuda', index=0)
```

如果设备序号不存在，即使在调用 torch.cuda.set_device()之后，该对象也将始终表示设备类型的当前设备。

*   **torch . layout:**A*torch . layout*是一个代表 torch.Tensor 内存布局的对象，目前该 torch 支持两种内存布局。

**1。torch.strided:** 代表密集张量，是最常用的内存布局。每个跨步张量都有一个相关的*火炬。存储，*保存其数据。这些张量提供了一个多维的、跨越式的存储视图。数组的步距(也称为增量、间距或步长)是连续数组元素开始之间在内存中的位置数，以字节或数组元素大小的单位来度量。跨距不能小于元素大小，但可以更大，表示元素之间有额外的空间。所以基本上，这里的步幅是一个整数列表:第 k 个步幅代表从张量第 k 维的一个元素跳到下一个元素所需的内存跳跃。这个概念使得有效地执行许多张量运算成为可能。

让我们运行一些示例片段:

## 蟒蛇 3

```
x = torch.Tensor([[1, 2, 3, 4], [5, 7, 8, 9]])
x.stride()
```

**输出:**

```
(4,1)
```

**2。torch.sparse_coo_tensor:** 用于存储稀疏坐标列表中的数组。在首席运营官格式中，指定的元素被存储为元素索引和相应值的元组。

## 蟒蛇 3

```
i = [[0, 1, 1],
     [2, 0, 2]]

v =  [3, 4, 5]
s = torch.sparse_coo_tensor(i, v, (2, 3))
Print(s)
```

**输出:**

```
tensor(indices=tensor([[0, 1, 1],
                       [2, 0, 2]]),
       values=tensor([3, 4, 5]),
       size=(2, 3), nnz=3, layout=torch.sparse_coo)
```

### **张量运算:**

你可以像矩阵加法一样加两个张量。

## 蟒蛇 3

```
x = torch.tensor([1., 2., 3.])
y = torch.tensor([4., 5., 6.])
z = x + y
print(z)
```

**输出:**

```
tensor([5., 7., 9.])
```

*   **torch.cat()** :连接张量列表

## 蟒蛇 3

```
x_1 = torch.randn(2, 5)
y_1 = torch.randn(3, 5)
z_1 = torch.cat([x_1, y_1])
print(z_1)
```

**输出:**

```
tensor([[ 0.5761,  0.6781,  0.1621,  0.4986,  0.3410],
        [-0.8428,  0.2510, -0.2668, -1.1475,  0.5675],
        [-0.2797, -0.0699,  2.8936,  1.8260,  2.1227],
        [ 1.3765, -0.0939, -0.3774, -0.3834,  0.0682],
        [ 2.3666,  0.0904,  0.7956,  1.2281,  0.5561]])
```

要连接列，可以执行以下操作。

## 蟒蛇 3

```
x_2 = torch.randn(2, 3)
y_2 = torch.randn(2, 5)

# second argument specifies which axis to concat along
z_2 = torch.cat([x_2, y_2], 1)
print(z_2)
```

**输出:**

```
tensor([[ 0.5818,  0.7047,  0.1581,  1.8658,  0.5953, -0.9453, -0.6395, -0.7106],
        [ 1.2197,  0.8110, -1.6072,  0.1463,  0.4895, -0.8226, -0.1889,  0.2668]])
```

*   **view():** 可以使用。view()方法如下所示。

## 蟒蛇 3

```
x = torch.randn(2, 3, 4)
print(x)

# reshape to 2 rows, 12 columns
print(x.view(2, 12))
```

**输出:**

```
tensor([[[ 0.4321,  0.2414, -0.4776,  1.6408],
         [ 0.9085,  0.9195,  0.1321,  1.1891],
         [-0.9267, -0.1384,  0.0115, -0.4731]],

        [[ 0.7256,  0.6990, -1.7374,  0.6053],
         [ 0.0224, -1.2108,  0.1974,  0.0655],
         [-0.6182, -0.0797,  0.2603, -1.3280]]])
tensor([[ 0.4321,  0.2414, -0.4776,  1.6408,  0.9085,  0.9195,  0.1321,  1.1891,
         -0.9267, -0.1384,  0.0115, -0.4731],
        [ 0.7256,  0.6990, -1.7374,  0.6053,  0.0224, -1.2108,  0.1974,  0.0655,
         -0.6182, -0.0797,  0.2603, -1.3280]])
```

*   **torch.argmax():** 返回输入张量中所有元素最大值的索引。

## 蟒蛇 3

```
x = torch.randn(3,3)
print((x, torch.argmax(x)))
```

**输出:**

```
(tensor([[ 1.9610, -0.7683, -2.6080],
        [-0.3659, -0.1731,  0.1061],
        [ 0.8582,  0.6420, -0.2380]]), tensor(0))
```

*   **torch.argmin():** 类似于 argmax()，它返回输入张量中所有元素的最小值。

## 蟒蛇 3

```
x = torch.randn(3,3)
print((x, torch.argmin(x)))
```

**输出:**

```
(tensor([[ 0.9838, -1.2761,  0.2257],
        [-0.4754,  1.2677,  1.1973],
        [-1.2298, -0.5710, -1.3635]]), tensor(8))
```